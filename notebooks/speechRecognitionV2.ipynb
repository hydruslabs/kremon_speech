{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMbeU7XUIA53",
    "outputId": "b0599a48-0607-4645-8a93-527f4f696623",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19_06_2023_07_41_44.pickle  INFERENCE_ENGINE.ipynb\n",
      "19_06_2023_08_42_29.pickle  __init__.py\n",
      "19_06_2023_16_58_47.pickle  speechRecognitionv1.ipynb\n",
      "dataLoadTest.ipynb          speechRecognitionV2.ipynb\n",
      "DATASET_CONSTRUCTOR.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-FFpKy1MwDw"
   },
   "source": [
    "Installation of Pydub for Audio File Manipulation and Analysis of audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWMJRzpHKedM",
    "outputId": "e418106a-7c1e-4b70-a749-328a1a8fba28",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.performanceMetrics import cer\n",
    "from utils.performanceMetrics import wer\n",
    "from utils.misc import IterMeter\n",
    "from utils.misc import GreedyDecoder\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from pydub import AudioSegment, effects\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import utils.transformations.textTransform as text_trans #????? \n",
    "\n",
    "import utils.networkConfigurations.SpeechRecognitionModel_0_0 as network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.local/lib/python3.9/site-packages/torchaudio/functional/functional.py:539: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "text_transform = text_trans.TextTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, utterance) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(torch.from_numpy(waveform).float()).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == 'valid':\n",
    "            spec = valid_audio_transforms(torch.from_numpy(waveform).float()).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            iter_meter.step()\n",
    "            train_loss += loss.item()/ len(train_loader)\n",
    "            \n",
    "            \n",
    "\n",
    "           # if batch_idx % 1 == 0 or data_len == ((batch_idx + 1) * 5) :\n",
    "            #    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "             #       epoch, (batch_idx + 1) * len(spectrograms), data_len,\n",
    "              #      100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "    \n",
    "    return epoch,train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, epoch, iter_meter):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "            for i, _data in enumerate(test_loader):\n",
    "                spectrograms, labels, input_lengths, label_lengths = _data \n",
    "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "                output = model(spectrograms)  # (batch, time, n_class)\n",
    "                output = F.log_softmax(output, dim=2)\n",
    "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "                test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "                for j in range(len(decoded_preds)):\n",
    "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "\n",
    "    avg_cer = sum(test_cer)/len(test_cer)\n",
    "    avg_wer = sum(test_wer)/len(test_wer)\n",
    "\n",
    "\n",
    "    return test_loss,avg_cer,avg_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_wer(wer_scores, combined_ref_len):\n",
    "    return float(sum(wer_scores)) / float(combined_ref_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_list = ['cherlaine_augmented_0.pickle',\n",
    "'cherlaine_augmented_100.pickle',\n",
    "'cherlaine_augmented_101.pickle',\n",
    "'cherlaine_augmented_102.pickle',\n",
    "'cherlaine_augmented_103.pickle',\n",
    "'cherlaine_augmented_104.pickle',\n",
    "'cherlaine_augmented_10.pickle',\n",
    "'cherlaine_augmented_11.pickle',\n",
    "'cherlaine_augmented_12.pickle',\n",
    "'cherlaine_augmented_13.pickle',\n",
    "'cherlaine_augmented_14.pickle',\n",
    "'cherlaine_augmented_15.pickle',\n",
    "'cherlaine_augmented_16.pickle',\n",
    "'cherlaine_augmented_17.pickle',\n",
    "'cherlaine_augmented_18.pickle',\n",
    "'cherlaine_augmented_19.pickle',\n",
    "'cherlaine_augmented_1.pickle',\n",
    "'cherlaine_augmented_20.pickle',\n",
    "'cherlaine_augmented_21.pickle',\n",
    "'cherlaine_augmented_22.pickle',\n",
    "'cherlaine_augmented_23.pickle',\n",
    "'cherlaine_augmented_24.pickle',\n",
    "'cherlaine_augmented_25.pickle',\n",
    "'cherlaine_augmented_26.pickle',\n",
    "'cherlaine_augmented_27.pickle',\n",
    "'cherlaine_augmented_28.pickle',\n",
    "'cherlaine_augmented_29.pickle',\n",
    "'cherlaine_augmented_2.pickle',\n",
    "'cherlaine_augmented_30.pickle',\n",
    "'cherlaine_augmented_31.pickle',\n",
    "'cherlaine_augmented_32.pickle',\n",
    "'cherlaine_augmented_33.pickle',\n",
    "'cherlaine_augmented_34.pickle',\n",
    "'cherlaine_augmented_35.pickle',\n",
    "'cherlaine_augmented_36.pickle',\n",
    "'cherlaine_augmented_37.pickle',\n",
    "'cherlaine_augmented_38.pickle',\n",
    "'cherlaine_augmented_39.pickle',\n",
    "'cherlaine_augmented_3.pickle',\n",
    "'cherlaine_augmented_40.pickle',\n",
    "'cherlaine_augmented_41.pickle',\n",
    "'cherlaine_augmented_42.pickle',\n",
    "'cherlaine_augmented_43.pickle',\n",
    "'cherlaine_augmented_44.pickle',\n",
    "'cherlaine_augmented_45.pickle',\n",
    "'cherlaine_augmented_46.pickle',\n",
    "'cherlaine_augmented_47.pickle',\n",
    "'cherlaine_augmented_48.pickle',\n",
    "'cherlaine_augmented_49.pickle',\n",
    "'cherlaine_augmented_4.pickle',\n",
    "'cherlaine_augmented_50.pickle',\n",
    "'cherlaine_augmented_51.pickle',\n",
    "'cherlaine_augmented_52.pickle',\n",
    "'cherlaine_augmented_53.pickle',\n",
    "'cherlaine_augmented_54.pickle',\n",
    "'cherlaine_augmented_55.pickle',\n",
    "'cherlaine_augmented_56.pickle',\n",
    "'cherlaine_augmented_57.pickle',\n",
    "'cherlaine_augmented_58.pickle',\n",
    "'cherlaine_augmented_59.pickle',\n",
    "'cherlaine_augmented_5.pickle',\n",
    "'cherlaine_augmented_60.pickle',\n",
    "'cherlaine_augmented_61.pickle',\n",
    "'cherlaine_augmented_62.pickle',\n",
    "'cherlaine_augmented_63.pickle',\n",
    "'cherlaine_augmented_64.pickle',\n",
    "'cherlaine_augmented_65.pickle',\n",
    "'cherlaine_augmented_66.pickle',\n",
    "'cherlaine_augmented_67.pickle',\n",
    "'cherlaine_augmented_68.pickle',\n",
    "'cherlaine_augmented_69.pickle',\n",
    "'cherlaine_augmented_6.pickle',\n",
    "'cherlaine_augmented_70.pickle',\n",
    "'cherlaine_augmented_71.pickle',\n",
    "'cherlaine_augmented_72.pickle',\n",
    "'cherlaine_augmented_73.pickle',\n",
    "'cherlaine_augmented_74.pickle',\n",
    "'cherlaine_augmented_75.pickle',\n",
    "'cherlaine_augmented_76.pickle',\n",
    "'cherlaine_augmented_77.pickle',\n",
    "'cherlaine_augmented_78.pickle',\n",
    "'cherlaine_augmented_79.pickle',\n",
    "'cherlaine_augmented_7.pickle',\n",
    "'cherlaine_augmented_80.pickle',\n",
    "'cherlaine_augmented_81.pickle',\n",
    "'cherlaine_augmented_82.pickle',\n",
    "'cherlaine_augmented_83.pickle',\n",
    "'cherlaine_augmented_84.pickle',\n",
    "'cherlaine_augmented_85.pickle',\n",
    "'cherlaine_augmented_86.pickle',\n",
    "'cherlaine_augmented_87.pickle',\n",
    "'cherlaine_augmented_88.pickle',\n",
    "'cherlaine_augmented_89.pickle',\n",
    "'cherlaine_augmented_8.pickle',\n",
    "'cherlaine_augmented_90.pickle',\n",
    "'cherlaine_augmented_91.pickle',\n",
    "'cherlaine_augmented_92.pickle',\n",
    "'cherlaine_augmented_93.pickle',\n",
    "'cherlaine_augmented_94.pickle',\n",
    "'cherlaine_augmented_95.pickle',\n",
    "'cherlaine_augmented_96.pickle',\n",
    "'cherlaine_augmented_97.pickle',\n",
    "'cherlaine_augmented_98.pickle',\n",
    "'cherlaine_augmented_99.pickle',\n",
    "'cherlaine_augmented_9.pickle',\n",
    "'kelly_augmented_0.pickle',\n",
    "'kelly_augmented_100.pickle',\n",
    "'kelly_augmented_101.pickle',\n",
    "'kelly_augmented_102.pickle',\n",
    "'kelly_augmented_103.pickle',\n",
    "'kelly_augmented_104.pickle',\n",
    "'kelly_augmented_105.pickle',\n",
    "'kelly_augmented_106.pickle',\n",
    "'kelly_augmented_107.pickle',\n",
    "'kelly_augmented_108.pickle',\n",
    "'kelly_augmented_109.pickle',\n",
    "'kelly_augmented_10.pickle',\n",
    "'kelly_augmented_110.pickle',\n",
    "'kelly_augmented_111.pickle',\n",
    "'kelly_augmented_112.pickle',\n",
    "'kelly_augmented_113.pickle',\n",
    "'kelly_augmented_114.pickle',\n",
    "'kelly_augmented_115.pickle',\n",
    "'kelly_augmented_116.pickle',\n",
    "'kelly_augmented_117.pickle',\n",
    "'kelly_augmented_118.pickle',\n",
    "'kelly_augmented_119.pickle',\n",
    "'kelly_augmented_11.pickle',\n",
    "'kelly_augmented_120.pickle',\n",
    "'kelly_augmented_121.pickle',\n",
    "'kelly_augmented_122.pickle',\n",
    "'kelly_augmented_123.pickle',\n",
    "'kelly_augmented_124.pickle',\n",
    "'kelly_augmented_125.pickle',\n",
    "'kelly_augmented_126.pickle',\n",
    "'kelly_augmented_127.pickle',\n",
    "'kelly_augmented_128.pickle',\n",
    "'kelly_augmented_129.pickle',\n",
    "'kelly_augmented_12.pickle',\n",
    "'kelly_augmented_130.pickle',\n",
    "'kelly_augmented_131.pickle',\n",
    "'kelly_augmented_132.pickle',\n",
    "'kelly_augmented_133.pickle',\n",
    "'kelly_augmented_134.pickle',\n",
    "'kelly_augmented_135.pickle',\n",
    "'kelly_augmented_136.pickle',\n",
    "'kelly_augmented_137.pickle',\n",
    "'kelly_augmented_138.pickle',\n",
    "'kelly_augmented_13.pickle',\n",
    "'kelly_augmented_14.pickle',\n",
    "'kelly_augmented_15.pickle',\n",
    "'kelly_augmented_16.pickle',\n",
    "'kelly_augmented_17.pickle',\n",
    "'kelly_augmented_18.pickle',\n",
    "'kelly_augmented_19.pickle',\n",
    "'kelly_augmented_1.pickle',\n",
    "'kelly_augmented_20.pickle',\n",
    "'kelly_augmented_21.pickle',\n",
    "'kelly_augmented_22.pickle',\n",
    "'kelly_augmented_23.pickle',\n",
    "'kelly_augmented_24.pickle',\n",
    "'kelly_augmented_25.pickle',\n",
    "'kelly_augmented_26.pickle',\n",
    "'kelly_augmented_27.pickle',\n",
    "'kelly_augmented_28.pickle',\n",
    "'kelly_augmented_29.pickle',\n",
    "'kelly_augmented_2.pickle',\n",
    "'kelly_augmented_30.pickle',\n",
    "'kelly_augmented_31.pickle',\n",
    "'kelly_augmented_32.pickle',\n",
    "'kelly_augmented_33.pickle',\n",
    "'kelly_augmented_34.pickle',\n",
    "'kelly_augmented_35.pickle',\n",
    "'kelly_augmented_36.pickle',\n",
    "'kelly_augmented_37.pickle',\n",
    "'kelly_augmented_38.pickle',\n",
    "'kelly_augmented_39.pickle',\n",
    "'kelly_augmented_3.pickle',\n",
    "'kelly_augmented_40.pickle',\n",
    "'kelly_augmented_41.pickle',\n",
    "'kelly_augmented_42.pickle',\n",
    "'kelly_augmented_43.pickle',\n",
    "'kelly_augmented_44.pickle',\n",
    "'kelly_augmented_45.pickle',\n",
    "'kelly_augmented_46.pickle',\n",
    "'kelly_augmented_47.pickle',\n",
    "'kelly_augmented_48.pickle',\n",
    "'kelly_augmented_49.pickle',\n",
    "'kelly_augmented_4.pickle',\n",
    "'kelly_augmented_50.pickle',\n",
    "'kelly_augmented_51.pickle',\n",
    "'kelly_augmented_52.pickle',\n",
    "'kelly_augmented_53.pickle',\n",
    "'kelly_augmented_54.pickle',\n",
    "'kelly_augmented_55.pickle',\n",
    "'kelly_augmented_56.pickle',\n",
    "'kelly_augmented_57.pickle',\n",
    "'kelly_augmented_58.pickle',\n",
    "'kelly_augmented_59.pickle',\n",
    "'kelly_augmented_5.pickle',\n",
    "'kelly_augmented_60.pickle',\n",
    "'kelly_augmented_61.pickle',\n",
    "'kelly_augmented_62.pickle',\n",
    "'kelly_augmented_63.pickle',\n",
    "'kelly_augmented_64.pickle',\n",
    "'kelly_augmented_65.pickle',\n",
    "'kelly_augmented_66.pickle',\n",
    "'kelly_augmented_67.pickle',\n",
    "'kelly_augmented_68.pickle',\n",
    "'kelly_augmented_69.pickle',\n",
    "'kelly_augmented_6.pickle',\n",
    "'kelly_augmented_70.pickle',\n",
    "'kelly_augmented_71.pickle',\n",
    "'kelly_augmented_72.pickle',\n",
    "'kelly_augmented_73.pickle',\n",
    "'kelly_augmented_74.pickle',\n",
    "'kelly_augmented_75.pickle',\n",
    "'kelly_augmented_76.pickle',\n",
    "'kelly_augmented_77.pickle',\n",
    "'kelly_augmented_78.pickle',\n",
    "'kelly_augmented_79.pickle',\n",
    "'kelly_augmented_7.pickle',\n",
    "'kelly_augmented_80.pickle',\n",
    "'kelly_augmented_81.pickle',\n",
    "'kelly_augmented_82.pickle',\n",
    "'kelly_augmented_83.pickle',\n",
    "'kelly_augmented_84.pickle',\n",
    "'kelly_augmented_85.pickle',\n",
    "'kelly_augmented_86.pickle',\n",
    "'kelly_augmented_87.pickle',\n",
    "'kelly_augmented_88.pickle',\n",
    "'kelly_augmented_89.pickle',\n",
    "'kelly_augmented_8.pickle',\n",
    "'kelly_augmented_90.pickle',\n",
    "'kelly_augmented_91.pickle',\n",
    "'kelly_augmented_92.pickle',\n",
    "'kelly_augmented_93.pickle',\n",
    "'kelly_augmented_94.pickle',\n",
    "'kelly_augmented_95.pickle',\n",
    "'kelly_augmented_96.pickle',\n",
    "'kelly_augmented_97.pickle',\n",
    "'kelly_augmented_98.pickle',\n",
    "'kelly_augmented_99.pickle',\n",
    "'kelly_augmented_9.pickle'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with: cherlaine_augmented_0.pickle\n",
      "Working with: cherlaine_augmented_100.pickle\n",
      "Working with: cherlaine_augmented_101.pickle\n",
      "Working with: cherlaine_augmented_102.pickle\n",
      "Working with: cherlaine_augmented_103.pickle\n",
      "Working with: cherlaine_augmented_104.pickle\n",
      "Working with: cherlaine_augmented_10.pickle\n",
      "Working with: cherlaine_augmented_11.pickle\n",
      "Working with: cherlaine_augmented_12.pickle\n",
      "Working with: cherlaine_augmented_13.pickle\n",
      "Working with: cherlaine_augmented_14.pickle\n",
      "Working with: cherlaine_augmented_15.pickle\n",
      "Working with: cherlaine_augmented_16.pickle\n",
      "Working with: cherlaine_augmented_17.pickle\n",
      "Working with: cherlaine_augmented_18.pickle\n",
      "Working with: cherlaine_augmented_19.pickle\n",
      "Working with: cherlaine_augmented_1.pickle\n",
      "Working with: cherlaine_augmented_20.pickle\n",
      "Working with: cherlaine_augmented_21.pickle\n",
      "Working with: cherlaine_augmented_22.pickle\n",
      "Working with: cherlaine_augmented_23.pickle\n",
      "Working with: cherlaine_augmented_24.pickle\n",
      "Working with: cherlaine_augmented_25.pickle\n",
      "Working with: cherlaine_augmented_26.pickle\n",
      "Working with: cherlaine_augmented_27.pickle\n",
      "Working with: cherlaine_augmented_28.pickle\n",
      "Working with: cherlaine_augmented_29.pickle\n",
      "Working with: cherlaine_augmented_2.pickle\n",
      "Working with: cherlaine_augmented_30.pickle\n",
      "Working with: cherlaine_augmented_31.pickle\n",
      "Working with: cherlaine_augmented_32.pickle\n",
      "Working with: cherlaine_augmented_33.pickle\n",
      "Working with: cherlaine_augmented_34.pickle\n",
      "Working with: cherlaine_augmented_35.pickle\n",
      "Working with: cherlaine_augmented_36.pickle\n",
      "Working with: cherlaine_augmented_37.pickle\n",
      "Working with: cherlaine_augmented_38.pickle\n",
      "Working with: cherlaine_augmented_39.pickle\n",
      "Working with: cherlaine_augmented_3.pickle\n",
      "Working with: cherlaine_augmented_40.pickle\n",
      "Working with: cherlaine_augmented_41.pickle\n",
      "Working with: cherlaine_augmented_42.pickle\n",
      "Working with: cherlaine_augmented_43.pickle\n",
      "Working with: cherlaine_augmented_44.pickle\n",
      "Working with: cherlaine_augmented_45.pickle\n",
      "Working with: cherlaine_augmented_46.pickle\n",
      "Working with: cherlaine_augmented_47.pickle\n",
      "Working with: cherlaine_augmented_48.pickle\n",
      "Working with: cherlaine_augmented_49.pickle\n",
      "Working with: cherlaine_augmented_4.pickle\n",
      "Working with: cherlaine_augmented_50.pickle\n",
      "Working with: cherlaine_augmented_51.pickle\n",
      "Working with: cherlaine_augmented_52.pickle\n",
      "Working with: cherlaine_augmented_53.pickle\n",
      "Working with: cherlaine_augmented_54.pickle\n",
      "Working with: cherlaine_augmented_55.pickle\n",
      "Working with: cherlaine_augmented_56.pickle\n",
      "Working with: cherlaine_augmented_57.pickle\n",
      "Working with: cherlaine_augmented_58.pickle\n",
      "Working with: cherlaine_augmented_59.pickle\n",
      "Working with: cherlaine_augmented_5.pickle\n",
      "Working with: cherlaine_augmented_60.pickle\n",
      "Working with: cherlaine_augmented_61.pickle\n",
      "Working with: cherlaine_augmented_62.pickle\n",
      "Working with: cherlaine_augmented_63.pickle\n",
      "Working with: cherlaine_augmented_64.pickle\n",
      "Working with: cherlaine_augmented_65.pickle\n",
      "Working with: cherlaine_augmented_66.pickle\n",
      "Working with: cherlaine_augmented_67.pickle\n",
      "Working with: cherlaine_augmented_68.pickle\n",
      "Working with: cherlaine_augmented_69.pickle\n",
      "Working with: cherlaine_augmented_6.pickle\n",
      "Working with: cherlaine_augmented_70.pickle\n",
      "Working with: cherlaine_augmented_71.pickle\n",
      "Working with: cherlaine_augmented_72.pickle\n",
      "Working with: cherlaine_augmented_73.pickle\n",
      "Working with: cherlaine_augmented_74.pickle\n",
      "Working with: cherlaine_augmented_75.pickle\n",
      "Working with: cherlaine_augmented_76.pickle\n",
      "Working with: cherlaine_augmented_77.pickle\n",
      "Working with: cherlaine_augmented_78.pickle\n",
      "Working with: cherlaine_augmented_79.pickle\n",
      "Working with: cherlaine_augmented_7.pickle\n",
      "Working with: cherlaine_augmented_80.pickle\n",
      "Working with: cherlaine_augmented_81.pickle\n",
      "Working with: cherlaine_augmented_82.pickle\n",
      "Working with: cherlaine_augmented_83.pickle\n",
      "Working with: cherlaine_augmented_84.pickle\n",
      "Working with: cherlaine_augmented_85.pickle\n",
      "Working with: cherlaine_augmented_86.pickle\n",
      "Working with: cherlaine_augmented_87.pickle\n",
      "Working with: cherlaine_augmented_88.pickle\n",
      "Working with: cherlaine_augmented_89.pickle\n",
      "Working with: cherlaine_augmented_8.pickle\n",
      "Working with: cherlaine_augmented_90.pickle\n",
      "Working with: cherlaine_augmented_91.pickle\n",
      "Working with: cherlaine_augmented_92.pickle\n",
      "Working with: cherlaine_augmented_93.pickle\n",
      "Working with: cherlaine_augmented_94.pickle\n",
      "Working with: cherlaine_augmented_95.pickle\n",
      "Working with: cherlaine_augmented_96.pickle\n",
      "Working with: cherlaine_augmented_97.pickle\n",
      "Working with: cherlaine_augmented_98.pickle\n",
      "Working with: cherlaine_augmented_99.pickle\n",
      "Working with: cherlaine_augmented_9.pickle\n",
      "Working with: kelly_augmented_0.pickle\n",
      "Working with: kelly_augmented_100.pickle\n",
      "Working with: kelly_augmented_101.pickle\n",
      "Working with: kelly_augmented_102.pickle\n",
      "Working with: kelly_augmented_103.pickle\n",
      "Working with: kelly_augmented_104.pickle\n",
      "Working with: kelly_augmented_105.pickle\n",
      "Working with: kelly_augmented_106.pickle\n",
      "Working with: kelly_augmented_107.pickle\n",
      "Working with: kelly_augmented_108.pickle\n",
      "Working with: kelly_augmented_109.pickle\n",
      "Working with: kelly_augmented_10.pickle\n",
      "Working with: kelly_augmented_110.pickle\n",
      "Working with: kelly_augmented_111.pickle\n",
      "Working with: kelly_augmented_112.pickle\n",
      "Working with: kelly_augmented_113.pickle\n",
      "Working with: kelly_augmented_114.pickle\n",
      "Working with: kelly_augmented_115.pickle\n",
      "Working with: kelly_augmented_116.pickle\n",
      "Working with: kelly_augmented_117.pickle\n",
      "Working with: kelly_augmented_118.pickle\n",
      "Working with: kelly_augmented_119.pickle\n",
      "Working with: kelly_augmented_11.pickle\n",
      "Working with: kelly_augmented_120.pickle\n",
      "Working with: kelly_augmented_121.pickle\n",
      "Working with: kelly_augmented_122.pickle\n",
      "Working with: kelly_augmented_123.pickle\n",
      "Working with: kelly_augmented_124.pickle\n",
      "Working with: kelly_augmented_125.pickle\n",
      "Working with: kelly_augmented_126.pickle\n",
      "Working with: kelly_augmented_127.pickle\n",
      "Working with: kelly_augmented_128.pickle\n",
      "Working with: kelly_augmented_129.pickle\n",
      "Working with: kelly_augmented_12.pickle\n",
      "Working with: kelly_augmented_130.pickle\n",
      "Working with: kelly_augmented_131.pickle\n",
      "Working with: kelly_augmented_132.pickle\n",
      "Working with: kelly_augmented_133.pickle\n",
      "Working with: kelly_augmented_134.pickle\n",
      "Working with: kelly_augmented_135.pickle\n",
      "Working with: kelly_augmented_136.pickle\n",
      "Working with: kelly_augmented_137.pickle\n",
      "Working with: kelly_augmented_138.pickle\n",
      "Working with: kelly_augmented_13.pickle\n",
      "Working with: kelly_augmented_14.pickle\n",
      "Working with: kelly_augmented_15.pickle\n",
      "Working with: kelly_augmented_16.pickle\n",
      "Working with: kelly_augmented_17.pickle\n",
      "Working with: kelly_augmented_18.pickle\n",
      "Working with: kelly_augmented_19.pickle\n",
      "Working with: kelly_augmented_1.pickle\n",
      "Working with: kelly_augmented_20.pickle\n",
      "Working with: kelly_augmented_21.pickle\n",
      "Working with: kelly_augmented_22.pickle\n",
      "Working with: kelly_augmented_23.pickle\n",
      "Working with: kelly_augmented_24.pickle\n",
      "Working with: kelly_augmented_25.pickle\n",
      "Working with: kelly_augmented_26.pickle\n",
      "Working with: kelly_augmented_27.pickle\n",
      "Working with: kelly_augmented_28.pickle\n",
      "Working with: kelly_augmented_29.pickle\n",
      "Working with: kelly_augmented_2.pickle\n",
      "Working with: kelly_augmented_30.pickle\n",
      "Working with: kelly_augmented_31.pickle\n",
      "Working with: kelly_augmented_32.pickle\n",
      "Working with: kelly_augmented_33.pickle\n",
      "Working with: kelly_augmented_34.pickle\n",
      "Working with: kelly_augmented_35.pickle\n",
      "Working with: kelly_augmented_36.pickle\n",
      "Working with: kelly_augmented_37.pickle\n",
      "Working with: kelly_augmented_38.pickle\n",
      "Working with: kelly_augmented_39.pickle\n",
      "Working with: kelly_augmented_3.pickle\n",
      "Working with: kelly_augmented_40.pickle\n",
      "Working with: kelly_augmented_41.pickle\n",
      "Working with: kelly_augmented_42.pickle\n",
      "Working with: kelly_augmented_43.pickle\n",
      "Working with: kelly_augmented_44.pickle\n",
      "Working with: kelly_augmented_45.pickle\n",
      "Working with: kelly_augmented_46.pickle\n",
      "Working with: kelly_augmented_47.pickle\n",
      "Working with: kelly_augmented_48.pickle\n",
      "Working with: kelly_augmented_49.pickle\n",
      "Working with: kelly_augmented_4.pickle\n",
      "Working with: kelly_augmented_50.pickle\n",
      "Working with: kelly_augmented_51.pickle\n",
      "Working with: kelly_augmented_52.pickle\n",
      "Working with: kelly_augmented_53.pickle\n",
      "Working with: kelly_augmented_54.pickle\n",
      "Working with: kelly_augmented_55.pickle\n",
      "Working with: kelly_augmented_56.pickle\n",
      "Working with: kelly_augmented_57.pickle\n",
      "Working with: kelly_augmented_58.pickle\n",
      "Working with: kelly_augmented_59.pickle\n",
      "Working with: kelly_augmented_5.pickle\n",
      "Working with: kelly_augmented_60.pickle\n",
      "Working with: kelly_augmented_61.pickle\n",
      "Working with: kelly_augmented_62.pickle\n",
      "Working with: kelly_augmented_63.pickle\n",
      "Working with: kelly_augmented_64.pickle\n",
      "Working with: kelly_augmented_65.pickle\n",
      "Working with: kelly_augmented_66.pickle\n",
      "Working with: kelly_augmented_67.pickle\n",
      "Working with: kelly_augmented_68.pickle\n",
      "Working with: kelly_augmented_69.pickle\n",
      "Working with: kelly_augmented_6.pickle\n",
      "Working with: kelly_augmented_70.pickle\n",
      "Working with: kelly_augmented_71.pickle\n",
      "Working with: kelly_augmented_72.pickle\n",
      "Working with: kelly_augmented_73.pickle\n",
      "Working with: kelly_augmented_74.pickle\n",
      "Working with: kelly_augmented_75.pickle\n",
      "Working with: kelly_augmented_76.pickle\n",
      "Working with: kelly_augmented_77.pickle\n",
      "Working with: kelly_augmented_78.pickle\n",
      "Working with: kelly_augmented_79.pickle\n",
      "Working with: kelly_augmented_7.pickle\n",
      "Working with: kelly_augmented_80.pickle\n",
      "Working with: kelly_augmented_81.pickle\n",
      "Working with: kelly_augmented_82.pickle\n",
      "Working with: kelly_augmented_83.pickle\n",
      "Working with: kelly_augmented_84.pickle\n",
      "Working with: kelly_augmented_85.pickle\n",
      "Working with: kelly_augmented_86.pickle\n",
      "Working with: kelly_augmented_87.pickle\n",
      "Working with: kelly_augmented_88.pickle\n",
      "Working with: kelly_augmented_89.pickle\n",
      "Working with: kelly_augmented_8.pickle\n",
      "Working with: kelly_augmented_90.pickle\n",
      "Working with: kelly_augmented_91.pickle\n",
      "Working with: kelly_augmented_92.pickle\n",
      "Working with: kelly_augmented_93.pickle\n",
      "Working with: kelly_augmented_94.pickle\n",
      "Working with: kelly_augmented_95.pickle\n",
      "Working with: kelly_augmented_96.pickle\n",
      "Working with: kelly_augmented_97.pickle\n",
      "Working with: kelly_augmented_98.pickle\n",
      "Working with: kelly_augmented_99.pickle\n",
      "Working with: kelly_augmented_9.pickle\n"
     ]
    }
   ],
   "source": [
    "b = []\n",
    "for filename in pickle_list:\n",
    "    print(\"Working with: \"+filename)\n",
    "    with open('/media/kremon-storage-space-2/dataset-03/'+filename, 'rb') as handle:\n",
    "        b = b + pickle.load(handle)\n",
    "        \n",
    "random.shuffle(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24400"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechRecognitionModel(\n",
      "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (rescnn_layers): Sequential(\n",
      "    (0): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualCNN(\n",
      "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm1): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (layer_norm2): CNNLayerNorm(\n",
      "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (birnn_layers): Sequential(\n",
      "    (0): BidirectionalGRU(\n",
      "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): BidirectionalGRU(\n",
      "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): GELU(approximate=none)\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
      "  )\n",
      ")\n",
      "{'n_cnn_layers': 3, 'n_rnn_layers': 5, 'rnn_dim': 512, 'n_class': 29, 'n_feats': 128, 'stride': 2, 'dropout': 0.1, 'learning_rate': 0.0005, 'batch_size': 50, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 2, 'pin_memory': True} \n",
    "\n",
    "hparams = {\n",
    "        \"n_cnn_layers\": 3,\n",
    "        \"n_rnn_layers\": 5,\n",
    "        \"rnn_dim\": 512,\n",
    "        \"n_class\": 29,\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\":2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"batch_size\": 50,\n",
    "        \"epochs\": 100\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_2_loader = data.DataLoader(dataset=b[0:24000],\n",
    "                                batch_size=hparams[\"batch_size\"],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "\n",
    "\n",
    "test_2_loader = data.DataLoader(dataset=b[24000:],\n",
    "                                batch_size=5,\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(7)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = network.SpeechRecognitionModel(\n",
    "            hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "            hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "            ).to(device)\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "#Test different optimizers \n",
    "#optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "#optimizer = optim.SGD(model.parameters(), lr = hparams['learning_rate'], momentum = 0.3)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hparams['learning_rate'])\n",
    "    \n",
    "\n",
    "criterion = nn.CTCLoss(blank=28).to(device)\n",
    "\n",
    "\n",
    "# Look into this learning rate scheduler seems odd?!\n",
    "# anneal strategy????\n",
    "\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                                steps_per_epoch=int(len(train_2_loader)),\n",
    "                                                epochs=hparams['epochs'],\n",
    "                                                anneal_strategy='linear')\n",
    "\n",
    "epochs = hparams['epochs']\n",
    "iter_meter = IterMeter()\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2023-12-02 18:28:09.189298\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 684.00 MiB (GPU 0; 47.54 GiB total capacity; 45.16 GiB already allocated; 306.69 MiB free; 45.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m dt_string \u001b[38;5;241m=\u001b[39m now\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m         epoch,train_loss\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_2_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_meter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         test_loss,avg_cer,avg_wer\u001b[38;5;241m=\u001b[39mtest(model, device, test_2_loader, criterion, epoch, iter_meter)\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Epoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: Average loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m Test set: Average loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m, Average CER: \u001b[39m\u001b[38;5;132;01m{:4f}\u001b[39;00m\u001b[38;5;124m Average WER: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch,train_loss,test_loss, avg_cer, avg_wer))\n",
      "Cell \u001b[0;32mIn [4], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\u001b[0m\n\u001b[1;32m      8\u001b[0m spectrograms, labels \u001b[38;5;241m=\u001b[39m spectrograms\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspectrograms\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch, time, n_class)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     14\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (time, batch, n_class)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/kremon-speech-recognition/notebooks/../utils/networkConfigurations/SpeechRecognitionModel_0_0.py:103\u001b[0m, in \u001b[0;36mSpeechRecognitionModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    102\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn(x)\n\u001b[0;32m--> 103\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrescnn_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     sizes \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    105\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(sizes[\u001b[38;5;241m0\u001b[39m], sizes[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m sizes[\u001b[38;5;241m2\u001b[39m], sizes[\u001b[38;5;241m3\u001b[39m])  \u001b[38;5;66;03m# (batch, feature, time)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/kremon-speech-recognition/notebooks/../utils/networkConfigurations/SpeechRecognitionModel_0_0.py:49\u001b[0m, in \u001b[0;36mResidualCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n\u001b[1;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn1(x)\n\u001b[0;32m---> 49\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgelu(x)\n\u001b[1;32m     51\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/kremon-speech-recognition/notebooks/../utils/networkConfigurations/SpeechRecognitionModel_0_0.py:25\u001b[0m, in \u001b[0;36mCNNLayerNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# x (batch, channel, feature, time)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous() \u001b[38;5;66;03m# (batch, channel, time, feature)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py:2503\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2501\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2502\u001b[0m     )\n\u001b[0;32m-> 2503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 684.00 MiB (GPU 0; 47.54 GiB total capacity; 45.16 GiB already allocated; 306.69 MiB free; 45.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "epoch_list = []\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "avg_cer_list = []\n",
    "avg_wer_list = []\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        epoch,train_loss=train(model, device, train_2_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n",
    "        test_loss,avg_cer,avg_wer=test(model, device, test_2_loader, criterion, epoch, iter_meter)\n",
    "        print('Train Epoch: {}: Average loss: {:.4f} Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}'.format(epoch,train_loss,test_loss, avg_cer, avg_wer))\n",
    "        \n",
    "        \n",
    "        \n",
    "        epoch_list.append(epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        avg_cer_list.append(avg_cer)\n",
    "        avg_wer_list.append(avg_wer)\n",
    "        output = np.vstack((np.array(epoch_list),np.array(train_loss_list),np.array(test_loss_list),np.array(avg_cer_list),np.array(avg_wer_list))).T\n",
    "        \n",
    "        \n",
    "\n",
    "        with open(dt_string+'.pickle', 'wb') as handle:\n",
    "             pickle.dump(output, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "        if epoch % 10 == 0:\n",
    "            print(\"-----------------------------------------------------\")\n",
    "            print(\"Saving Neural Network State\"+\"model_\"+dt_string+\"_\"+str(epoch)+\".pt\")\n",
    "            print(\"-----------------------------------------------------\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss\n",
    "                }, \"/media/kremon-storage-space-2/models\"+dt_string+\"_\"+str(epoch)+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../models/\"+\"model_\"+dt_string+\"_\"+str(10)+\".pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepocessed_data=b[60:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = data.DataLoader(dataset=prepocessed_data,\n",
    "                                batch_size=1,\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, _data in enumerate(valid_loader):\n",
    "    print(i)\n",
    "    spectrograms, labels, input_lengths, label_lengths = _data\n",
    "    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "    output = model(spectrograms)  # (batch, time, n_class)\n",
    "    output = F.log_softmax(output, dim=2)\n",
    "    output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "    decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "    print(decoded_preds)\n",
    "    print(decoded_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list=[1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform.int_to_text(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "            for i, _data in enumerate(test_loader):\n",
    "                spectrograms, labels, input_lengths, label_lengths = _data \n",
    "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "                output = model(spectrograms)  # (batch, time, n_class)\n",
    "                output = F.log_softmax(output, dim=2)\n",
    "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "                test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "                for j in range(len(decoded_preds)):\n",
    "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "\n",
    "    avg_cer = sum(test_cer)/len(test_cer)\n",
    "    avg_wer = sum(test_wer)/len(test_wer)\n",
    "\n",
    "\n",
    "    return test_loss,avg_cer,avg_wer"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
