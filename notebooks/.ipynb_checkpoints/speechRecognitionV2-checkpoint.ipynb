{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMbeU7XUIA53",
    "outputId": "b0599a48-0607-4645-8a93-527f4f696623",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mdataLoadTest.ipynb\u001b[0m*         \u001b[01;32mINFERENCE_ENGINE.ipynb\u001b[0m*  \u001b[01;32mspeechRecognitionv1.ipynb\u001b[0m*\n",
      "\u001b[01;32mDATASET_CONSTRUCTOR.ipynb\u001b[0m*  \u001b[01;32m__init__.py\u001b[0m*             \u001b[01;32mspeechRecognitionV2.ipynb\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-FFpKy1MwDw"
   },
   "source": [
    "Installation of Pydub for Audio File Manipulation and Analysis of audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWMJRzpHKedM",
    "outputId": "e418106a-7c1e-4b70-a749-328a1a8fba28",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.performanceMetrics import cer\n",
    "from utils.performanceMetrics import wer\n",
    "from utils.misc import IterMeter\n",
    "from utils.misc import GreedyDecoder\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from pydub import AudioSegment, effects\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import utils.transformations.textTransform as text_trans #????? \n",
    "\n",
    "import utils.networkConfigurations.SpeechRecognitionModel_1_0 as network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/.local/lib/python3.9/site-packages/torchaudio/functional/functional.py:539: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "text_transform = text_trans.TextTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, utterance) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(torch.from_numpy(waveform).float()).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == 'valid':\n",
    "            spec = valid_audio_transforms(torch.from_numpy(waveform).float()).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            iter_meter.step()\n",
    "            train_loss += loss.item()/ len(train_loader)\n",
    "            \n",
    "            \n",
    "\n",
    "           # if batch_idx % 1 == 0 or data_len == ((batch_idx + 1) * 5) :\n",
    "            #    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "             #       epoch, (batch_idx + 1) * len(spectrograms), data_len,\n",
    "              #      100. * batch_idx / len(train_loader), loss.item()))\n",
    "                \n",
    "    \n",
    "    return epoch,train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, epoch, iter_meter):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "            for i, _data in enumerate(test_loader):\n",
    "                spectrograms, labels, input_lengths, label_lengths = _data \n",
    "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "                output = model(spectrograms)  # (batch, time, n_class)\n",
    "                output = F.log_softmax(output, dim=2)\n",
    "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "                test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "                for j in range(len(decoded_preds)):\n",
    "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "\n",
    "    avg_cer = sum(test_cer)/len(test_cer)\n",
    "    avg_wer = sum(test_wer)/len(test_wer)\n",
    "\n",
    "\n",
    "    return test_loss,avg_cer,avg_wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_wer(wer_scores, combined_ref_len):\n",
    "    return float(sum(wer_scores)) / float(combined_ref_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_list = ['cherlaine_augmented_0.pickle'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle_list = ['cherlaine_augmented_0.pickle',\n",
    "'cherlaine_augmented_100.pickle',\n",
    "'cherlaine_augmented_101.pickle',\n",
    "'cherlaine_augmented_102.pickle',\n",
    "'cherlaine_augmented_103.pickle',\n",
    "'cherlaine_augmented_104.pickle',\n",
    "'cherlaine_augmented_10.pickle',\n",
    "'cherlaine_augmented_11.pickle',\n",
    "'cherlaine_augmented_12.pickle',\n",
    "'cherlaine_augmented_13.pickle',\n",
    "'cherlaine_augmented_14.pickle',\n",
    "'cherlaine_augmented_15.pickle',\n",
    "'cherlaine_augmented_16.pickle',\n",
    "'cherlaine_augmented_17.pickle',\n",
    "'cherlaine_augmented_18.pickle',\n",
    "'cherlaine_augmented_19.pickle',\n",
    "'cherlaine_augmented_1.pickle',\n",
    "'cherlaine_augmented_20.pickle',\n",
    "'cherlaine_augmented_21.pickle',\n",
    "'cherlaine_augmented_22.pickle',\n",
    "'cherlaine_augmented_23.pickle',\n",
    "'cherlaine_augmented_24.pickle',\n",
    "'cherlaine_augmented_25.pickle',\n",
    "'cherlaine_augmented_26.pickle',\n",
    "'cherlaine_augmented_27.pickle',\n",
    "'cherlaine_augmented_28.pickle',\n",
    "'cherlaine_augmented_29.pickle',\n",
    "'cherlaine_augmented_2.pickle',\n",
    "'cherlaine_augmented_30.pickle',\n",
    "'cherlaine_augmented_31.pickle',\n",
    "'cherlaine_augmented_32.pickle',\n",
    "'cherlaine_augmented_33.pickle',\n",
    "'cherlaine_augmented_34.pickle',\n",
    "'cherlaine_augmented_35.pickle',\n",
    "'cherlaine_augmented_36.pickle',\n",
    "'cherlaine_augmented_37.pickle',\n",
    "'cherlaine_augmented_38.pickle',\n",
    "'cherlaine_augmented_39.pickle',\n",
    "'cherlaine_augmented_3.pickle',\n",
    "'cherlaine_augmented_40.pickle',\n",
    "'cherlaine_augmented_41.pickle',\n",
    "'cherlaine_augmented_42.pickle',\n",
    "'cherlaine_augmented_43.pickle',\n",
    "'cherlaine_augmented_44.pickle',\n",
    "'cherlaine_augmented_45.pickle',\n",
    "'cherlaine_augmented_46.pickle',\n",
    "'cherlaine_augmented_47.pickle',\n",
    "'cherlaine_augmented_48.pickle',\n",
    "'cherlaine_augmented_49.pickle',\n",
    "'cherlaine_augmented_4.pickle',\n",
    "'cherlaine_augmented_50.pickle',\n",
    "'cherlaine_augmented_51.pickle',\n",
    "'cherlaine_augmented_52.pickle',\n",
    "'cherlaine_augmented_53.pickle',\n",
    "'cherlaine_augmented_54.pickle',\n",
    "'cherlaine_augmented_55.pickle',\n",
    "'cherlaine_augmented_56.pickle',\n",
    "'cherlaine_augmented_57.pickle',\n",
    "'cherlaine_augmented_58.pickle',\n",
    "'cherlaine_augmented_59.pickle',\n",
    "'cherlaine_augmented_5.pickle',\n",
    "'cherlaine_augmented_60.pickle',\n",
    "'cherlaine_augmented_61.pickle',\n",
    "'cherlaine_augmented_62.pickle',\n",
    "'cherlaine_augmented_63.pickle',\n",
    "'cherlaine_augmented_64.pickle',\n",
    "'cherlaine_augmented_65.pickle',\n",
    "'cherlaine_augmented_66.pickle',\n",
    "'cherlaine_augmented_67.pickle',\n",
    "'cherlaine_augmented_68.pickle',\n",
    "'cherlaine_augmented_69.pickle',\n",
    "'cherlaine_augmented_6.pickle',\n",
    "'cherlaine_augmented_70.pickle',\n",
    "'cherlaine_augmented_71.pickle',\n",
    "'cherlaine_augmented_72.pickle',\n",
    "'cherlaine_augmented_73.pickle',\n",
    "'cherlaine_augmented_74.pickle',\n",
    "'cherlaine_augmented_75.pickle',\n",
    "'cherlaine_augmented_76.pickle',\n",
    "'cherlaine_augmented_77.pickle',\n",
    "'cherlaine_augmented_78.pickle',\n",
    "'cherlaine_augmented_79.pickle',\n",
    "'cherlaine_augmented_7.pickle',\n",
    "'cherlaine_augmented_80.pickle',\n",
    "'cherlaine_augmented_81.pickle',\n",
    "'cherlaine_augmented_82.pickle',\n",
    "'cherlaine_augmented_83.pickle',\n",
    "'cherlaine_augmented_84.pickle',\n",
    "'cherlaine_augmented_85.pickle',\n",
    "'cherlaine_augmented_86.pickle',\n",
    "'cherlaine_augmented_87.pickle',\n",
    "'cherlaine_augmented_88.pickle',\n",
    "'cherlaine_augmented_89.pickle',\n",
    "'cherlaine_augmented_8.pickle',\n",
    "'cherlaine_augmented_90.pickle',\n",
    "'cherlaine_augmented_91.pickle',\n",
    "'cherlaine_augmented_92.pickle',\n",
    "'cherlaine_augmented_93.pickle',\n",
    "'cherlaine_augmented_94.pickle',\n",
    "'cherlaine_augmented_95.pickle',\n",
    "'cherlaine_augmented_96.pickle',\n",
    "'cherlaine_augmented_97.pickle',\n",
    "'cherlaine_augmented_98.pickle',\n",
    "'cherlaine_augmented_99.pickle',\n",
    "'cherlaine_augmented_9.pickle',\n",
    "'kelly_augmented_0.pickle',\n",
    "'kelly_augmented_100.pickle',\n",
    "'kelly_augmented_101.pickle',\n",
    "'kelly_augmented_102.pickle',\n",
    "'kelly_augmented_103.pickle',\n",
    "'kelly_augmented_104.pickle',\n",
    "'kelly_augmented_105.pickle',\n",
    "'kelly_augmented_106.pickle',\n",
    "'kelly_augmented_107.pickle',\n",
    "'kelly_augmented_108.pickle',\n",
    "'kelly_augmented_109.pickle',\n",
    "'kelly_augmented_10.pickle',\n",
    "'kelly_augmented_110.pickle',\n",
    "'kelly_augmented_111.pickle',\n",
    "'kelly_augmented_112.pickle',\n",
    "'kelly_augmented_113.pickle',\n",
    "'kelly_augmented_114.pickle',\n",
    "'kelly_augmented_115.pickle',\n",
    "'kelly_augmented_116.pickle',\n",
    "'kelly_augmented_117.pickle',\n",
    "'kelly_augmented_118.pickle',\n",
    "'kelly_augmented_119.pickle',\n",
    "'kelly_augmented_11.pickle',\n",
    "'kelly_augmented_120.pickle',\n",
    "'kelly_augmented_121.pickle',\n",
    "'kelly_augmented_122.pickle',\n",
    "'kelly_augmented_123.pickle',\n",
    "'kelly_augmented_124.pickle',\n",
    "'kelly_augmented_125.pickle',\n",
    "'kelly_augmented_126.pickle',\n",
    "'kelly_augmented_127.pickle',\n",
    "'kelly_augmented_128.pickle',\n",
    "'kelly_augmented_129.pickle',\n",
    "'kelly_augmented_12.pickle',\n",
    "'kelly_augmented_130.pickle',\n",
    "'kelly_augmented_131.pickle',\n",
    "'kelly_augmented_132.pickle',\n",
    "'kelly_augmented_133.pickle',\n",
    "'kelly_augmented_134.pickle',\n",
    "'kelly_augmented_135.pickle',\n",
    "'kelly_augmented_136.pickle',\n",
    "'kelly_augmented_137.pickle',\n",
    "'kelly_augmented_138.pickle',\n",
    "'kelly_augmented_13.pickle',\n",
    "'kelly_augmented_14.pickle',\n",
    "'kelly_augmented_15.pickle',\n",
    "'kelly_augmented_16.pickle',\n",
    "'kelly_augmented_17.pickle',\n",
    "'kelly_augmented_18.pickle',\n",
    "'kelly_augmented_19.pickle',\n",
    "'kelly_augmented_1.pickle',\n",
    "'kelly_augmented_20.pickle',\n",
    "'kelly_augmented_21.pickle',\n",
    "'kelly_augmented_22.pickle',\n",
    "'kelly_augmented_23.pickle',\n",
    "'kelly_augmented_24.pickle',\n",
    "'kelly_augmented_25.pickle',\n",
    "'kelly_augmented_26.pickle',\n",
    "'kelly_augmented_27.pickle',\n",
    "'kelly_augmented_28.pickle',\n",
    "'kelly_augmented_29.pickle',\n",
    "'kelly_augmented_2.pickle',\n",
    "'kelly_augmented_30.pickle',\n",
    "'kelly_augmented_31.pickle',\n",
    "'kelly_augmented_32.pickle',\n",
    "'kelly_augmented_33.pickle',\n",
    "'kelly_augmented_34.pickle',\n",
    "'kelly_augmented_35.pickle',\n",
    "'kelly_augmented_36.pickle',\n",
    "'kelly_augmented_37.pickle',\n",
    "'kelly_augmented_38.pickle',\n",
    "'kelly_augmented_39.pickle',\n",
    "'kelly_augmented_3.pickle',\n",
    "'kelly_augmented_40.pickle',\n",
    "'kelly_augmented_41.pickle',\n",
    "'kelly_augmented_42.pickle',\n",
    "'kelly_augmented_43.pickle',\n",
    "'kelly_augmented_44.pickle',\n",
    "'kelly_augmented_45.pickle',\n",
    "'kelly_augmented_46.pickle',\n",
    "'kelly_augmented_47.pickle',\n",
    "'kelly_augmented_48.pickle',\n",
    "'kelly_augmented_49.pickle',\n",
    "'kelly_augmented_4.pickle',\n",
    "'kelly_augmented_50.pickle',\n",
    "'kelly_augmented_51.pickle',\n",
    "'kelly_augmented_52.pickle',\n",
    "'kelly_augmented_53.pickle',\n",
    "'kelly_augmented_54.pickle',\n",
    "'kelly_augmented_55.pickle',\n",
    "'kelly_augmented_56.pickle',\n",
    "'kelly_augmented_57.pickle',\n",
    "'kelly_augmented_58.pickle',\n",
    "'kelly_augmented_59.pickle',\n",
    "'kelly_augmented_5.pickle',\n",
    "'kelly_augmented_60.pickle',\n",
    "'kelly_augmented_61.pickle',\n",
    "'kelly_augmented_62.pickle',\n",
    "'kelly_augmented_63.pickle',\n",
    "'kelly_augmented_64.pickle',\n",
    "'kelly_augmented_65.pickle',\n",
    "'kelly_augmented_66.pickle',\n",
    "'kelly_augmented_67.pickle',\n",
    "'kelly_augmented_68.pickle',\n",
    "'kelly_augmented_69.pickle',\n",
    "'kelly_augmented_6.pickle',\n",
    "'kelly_augmented_70.pickle',\n",
    "'kelly_augmented_71.pickle',\n",
    "'kelly_augmented_72.pickle',\n",
    "'kelly_augmented_73.pickle',\n",
    "'kelly_augmented_74.pickle',\n",
    "'kelly_augmented_75.pickle',\n",
    "'kelly_augmented_76.pickle',\n",
    "'kelly_augmented_77.pickle',\n",
    "'kelly_augmented_78.pickle',\n",
    "'kelly_augmented_79.pickle',\n",
    "'kelly_augmented_7.pickle',\n",
    "'kelly_augmented_80.pickle',\n",
    "'kelly_augmented_81.pickle',\n",
    "'kelly_augmented_82.pickle',\n",
    "'kelly_augmented_83.pickle',\n",
    "'kelly_augmented_84.pickle',\n",
    "'kelly_augmented_85.pickle',\n",
    "'kelly_augmented_86.pickle',\n",
    "'kelly_augmented_87.pickle',\n",
    "'kelly_augmented_88.pickle',\n",
    "'kelly_augmented_89.pickle',\n",
    "'kelly_augmented_8.pickle',\n",
    "'kelly_augmented_90.pickle',\n",
    "'kelly_augmented_91.pickle',\n",
    "'kelly_augmented_92.pickle',\n",
    "'kelly_augmented_93.pickle',\n",
    "'kelly_augmented_94.pickle',\n",
    "'kelly_augmented_95.pickle',\n",
    "'kelly_augmented_96.pickle',\n",
    "'kelly_augmented_97.pickle',\n",
    "'kelly_augmented_98.pickle',\n",
    "'kelly_augmented_99.pickle',\n",
    "'kelly_augmented_9.pickle'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with: cherlaine_augmented_0.pickle\n",
      "Working with: cherlaine_augmented_100.pickle\n"
     ]
    }
   ],
   "source": [
    "b = []\n",
    "for filename in pickle_list:\n",
    "    print(\"Working with: \"+filename)\n",
    "    with open('/media/kremon-storage-space-2/dataset-03/'+filename, 'rb') as handle:\n",
    "        b = b + pickle.load(handle)\n",
    "        \n",
    "random.shuffle(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpeechRecognitionModel(\n",
      "  (cnn): Conv2d(1, 32, kernel_size=(41, 11), stride=(2, 2), padding=(20, 20))\n",
      "  (batch_norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout01): Dropout(p=0.1, inplace=False)\n",
      "  (rescnn_layer1): ResidualCNN(\n",
      "    (cnn01): Conv2d(32, 32, kernel_size=(7, 3), stride=(1, 1), padding=(3, 1))\n",
      "    (cnn02): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (batch_norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout02): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (rescnn_layer2): ResidualCNN(\n",
      "    (cnn01): Conv2d(32, 32, kernel_size=(5, 3), stride=(1, 1), padding=(2, 1))\n",
      "    (cnn02): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (batch_norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout02): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (rescnn_layer3): ResidualCNN(\n",
      "    (cnn01): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (cnn02): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (batch_norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout02): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (rescnn_layer4): ResidualCNN(\n",
      "    (cnn01): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (cnn02): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (batch_norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout02): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (rescnn_layer5): ResidualCNN(\n",
      "    (cnn01): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (cnn02): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (batch_norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (batch_norm4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout02): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (fully_connected): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (birnn_layer1): BidirectionalGRU(\n",
      "    (BiGRU): GRU(1024, 1024, batch_first=True, bidirectional=True)\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout04): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (birnn_layer2): BidirectionalGRU(\n",
      "    (BiGRU): GRU(2048, 1024, bidirectional=True)\n",
      "    (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout04): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (birnn_layer3): BidirectionalGRU(\n",
      "    (BiGRU): GRU(2048, 1024, bidirectional=True)\n",
      "    (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout04): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (birnn_layer4): BidirectionalGRU(\n",
      "    (BiGRU): GRU(2048, 1024, bidirectional=True)\n",
      "    (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout04): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (1): GELU(approximate=none)\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=1024, out_features=29, bias=True)\n",
      "  )\n",
      ")\n",
      "{'n_cnn_layers': 3, 'n_rnn_layers': 5, 'rnn_dim': 512, 'n_class': 29, 'n_feats': 128, 'stride': 2, 'dropout': 0.1, 'learning_rate': 0.0005, 'batch_size': 5, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 2, 'pin_memory': True} \n",
    "\n",
    "hparams = {\n",
    "        \"n_cnn_layers\": 3,\n",
    "        \"n_rnn_layers\": 5,\n",
    "        \"rnn_dim\": 512,\n",
    "        \"n_class\": 29,\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\":2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"batch_size\": 5,\n",
    "        \"epochs\": 100\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_2_loader = data.DataLoader(dataset=b[0:150],\n",
    "                                batch_size=hparams[\"batch_size\"],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "\n",
    "\n",
    "test_2_loader = data.DataLoader(dataset=b[150:],\n",
    "                                batch_size=5,\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(7)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = network.SpeechRecognitionModel().to(device)\n",
    "\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "#Test different optimizers \n",
    "#optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "#optimizer = optim.SGD(model.parameters(), lr = hparams['learning_rate'], momentum = 0.3)\n",
    "\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=hparams['learning_rate'])\n",
    "    \n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1, momentum=0.95, dampening=0, weight_decay=0, nesterov=False)\n",
    "\n",
    "criterion = nn.CTCLoss(blank=28).to(device)\n",
    "\n",
    "\n",
    "# Look into this learning rate scheduler seems odd?!\n",
    "# anneal strategy????\n",
    "\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                               steps_per_epoch=int(len(train_2_loader)),\n",
    "                                               epochs=hparams['epochs'],\n",
    "                                               anneal_strategy='linear')\n",
    "\n",
    "\n",
    "\n",
    "epochs = hparams['epochs']\n",
    "iter_meter = IterMeter()\n",
    "print(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now = 2023-12-04 17:46:09.227680\n",
      "torch.Size([5, 32, 64, 895])\n",
      "torch.Size([5, 895, 2048])\n",
      "torch.Size([5, 32, 64, 1152])\n",
      "torch.Size([5, 1152, 2048])\n",
      "torch.Size([5, 32, 64, 1254])\n",
      "torch.Size([5, 1254, 2048])\n",
      "torch.Size([5, 32, 64, 1440])\n",
      "torch.Size([5, 1440, 2048])\n",
      "torch.Size([5, 32, 64, 1278])\n",
      "torch.Size([5, 1278, 2048])\n",
      "torch.Size([5, 32, 64, 1371])\n",
      "torch.Size([5, 1371, 2048])\n",
      "torch.Size([5, 32, 64, 931])\n",
      "torch.Size([5, 931, 2048])\n",
      "torch.Size([5, 32, 64, 1416])\n",
      "torch.Size([5, 1416, 2048])\n",
      "torch.Size([5, 32, 64, 1215])\n",
      "torch.Size([5, 1215, 2048])\n",
      "torch.Size([5, 32, 64, 1192])\n",
      "torch.Size([5, 1192, 2048])\n",
      "torch.Size([5, 32, 64, 1282])\n",
      "torch.Size([5, 1282, 2048])\n",
      "torch.Size([5, 32, 64, 1100])\n",
      "torch.Size([5, 1100, 2048])\n",
      "torch.Size([5, 32, 64, 999])\n",
      "torch.Size([5, 999, 2048])\n",
      "torch.Size([5, 32, 64, 1048])\n",
      "torch.Size([5, 1048, 2048])\n",
      "torch.Size([5, 32, 64, 1124])\n",
      "torch.Size([5, 1124, 2048])\n",
      "torch.Size([5, 32, 64, 1083])\n",
      "torch.Size([5, 1083, 2048])\n",
      "torch.Size([5, 32, 64, 1244])\n",
      "torch.Size([5, 1244, 2048])\n",
      "torch.Size([5, 32, 64, 1277])\n",
      "torch.Size([5, 1277, 2048])\n",
      "torch.Size([5, 32, 64, 1193])\n",
      "torch.Size([5, 1193, 2048])\n",
      "torch.Size([5, 32, 64, 984])\n",
      "torch.Size([5, 984, 2048])\n",
      "torch.Size([5, 32, 64, 872])\n",
      "torch.Size([5, 872, 2048])\n",
      "torch.Size([5, 32, 64, 1556])\n",
      "torch.Size([5, 1556, 2048])\n",
      "torch.Size([5, 32, 64, 1201])\n",
      "torch.Size([5, 1201, 2048])\n",
      "torch.Size([5, 32, 64, 1437])\n",
      "torch.Size([5, 1437, 2048])\n",
      "torch.Size([5, 32, 64, 850])\n",
      "torch.Size([5, 850, 2048])\n",
      "torch.Size([5, 32, 64, 1007])\n",
      "torch.Size([5, 1007, 2048])\n",
      "torch.Size([5, 32, 64, 1165])\n",
      "torch.Size([5, 1165, 2048])\n",
      "torch.Size([5, 32, 64, 1346])\n",
      "torch.Size([5, 1346, 2048])\n",
      "torch.Size([5, 32, 64, 1457])\n",
      "torch.Size([5, 1457, 2048])\n",
      "torch.Size([5, 32, 64, 1244])\n",
      "torch.Size([5, 1244, 2048])\n",
      "torch.Size([5, 32, 64, 1130])\n",
      "torch.Size([5, 1130, 2048])\n",
      "torch.Size([5, 32, 64, 1149])\n",
      "torch.Size([5, 1149, 2048])\n",
      "torch.Size([5, 32, 64, 1280])\n",
      "torch.Size([5, 1280, 2048])\n",
      "torch.Size([5, 32, 64, 1054])\n",
      "torch.Size([5, 1054, 2048])\n",
      "torch.Size([5, 32, 64, 1356])\n",
      "torch.Size([5, 1356, 2048])\n",
      "torch.Size([5, 32, 64, 1301])\n",
      "torch.Size([5, 1301, 2048])\n",
      "torch.Size([5, 32, 64, 1358])\n",
      "torch.Size([5, 1358, 2048])\n",
      "torch.Size([5, 32, 64, 1124])\n",
      "torch.Size([5, 1124, 2048])\n",
      "torch.Size([5, 32, 64, 1070])\n",
      "torch.Size([5, 1070, 2048])\n",
      "torch.Size([5, 32, 64, 796])\n",
      "torch.Size([5, 796, 2048])\n",
      "Train Epoch: 1: Average loss: 15.2313 Test set: Average loss: 11.0309, Average CER: 1.000000 Average WER: 0.9658\n",
      "torch.Size([5, 32, 64, 1067])\n",
      "torch.Size([5, 1067, 2048])\n",
      "torch.Size([5, 32, 64, 1278])\n",
      "torch.Size([5, 1278, 2048])\n",
      "torch.Size([5, 32, 64, 1081])\n",
      "torch.Size([5, 1081, 2048])\n",
      "torch.Size([5, 32, 64, 1437])\n",
      "torch.Size([5, 1437, 2048])\n",
      "torch.Size([5, 32, 64, 984])\n",
      "torch.Size([5, 984, 2048])\n",
      "torch.Size([5, 32, 64, 1244])\n",
      "torch.Size([5, 1244, 2048])\n",
      "torch.Size([5, 32, 64, 938])\n",
      "torch.Size([5, 938, 2048])\n",
      "torch.Size([5, 32, 64, 1192])\n",
      "torch.Size([5, 1192, 2048])\n",
      "torch.Size([5, 32, 64, 1556])\n",
      "torch.Size([5, 1556, 2048])\n",
      "torch.Size([5, 32, 64, 1048])\n",
      "torch.Size([5, 1048, 2048])\n",
      "torch.Size([5, 32, 64, 1059])\n",
      "torch.Size([5, 1059, 2048])\n",
      "torch.Size([5, 32, 64, 1215])\n",
      "torch.Size([5, 1215, 2048])\n",
      "torch.Size([5, 32, 64, 1282])\n",
      "torch.Size([5, 1282, 2048])\n",
      "torch.Size([5, 32, 64, 993])\n",
      "torch.Size([5, 993, 2048])\n",
      "torch.Size([5, 32, 64, 1181])\n",
      "torch.Size([5, 1181, 2048])\n",
      "torch.Size([5, 32, 64, 1440])\n",
      "torch.Size([5, 1440, 2048])\n",
      "torch.Size([5, 32, 64, 999])\n",
      "torch.Size([5, 999, 2048])\n",
      "torch.Size([5, 32, 64, 1254])\n",
      "torch.Size([5, 1254, 2048])\n",
      "torch.Size([5, 32, 64, 931])\n",
      "torch.Size([5, 931, 2048])\n",
      "torch.Size([5, 32, 64, 1371])\n",
      "torch.Size([5, 1371, 2048])\n",
      "torch.Size([5, 32, 64, 1277])\n",
      "torch.Size([5, 1277, 2048])\n",
      "torch.Size([5, 32, 64, 1346])\n",
      "torch.Size([5, 1346, 2048])\n",
      "torch.Size([5, 32, 64, 931])\n",
      "torch.Size([5, 931, 2048])\n",
      "torch.Size([5, 32, 64, 984])\n",
      "torch.Size([5, 984, 2048])\n",
      "torch.Size([5, 32, 64, 1244])\n",
      "torch.Size([5, 1244, 2048])\n",
      "torch.Size([5, 32, 64, 1046])\n",
      "torch.Size([5, 1046, 2048])\n",
      "torch.Size([5, 32, 64, 1083])\n",
      "torch.Size([5, 1083, 2048])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m dt_string \u001b[38;5;241m=\u001b[39m now\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m         epoch,train_loss\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_2_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_meter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         test_loss,avg_cer,avg_wer\u001b[38;5;241m=\u001b[39mtest(model, device, test_2_loader, criterion, epoch, iter_meter)\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Epoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: Average loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m Test set: Average loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m, Average CER: \u001b[39m\u001b[38;5;132;01m{:4f}\u001b[39;00m\u001b[38;5;124m Average WER: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch,train_loss,test_loss, avg_cer, avg_wer))\n",
      "Cell \u001b[0;32mIn [4], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\u001b[0m\n\u001b[1;32m     14\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (time, batch, n_class)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels, input_lengths, label_lengths)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_list = []\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "avg_cer_list = []\n",
    "avg_wer_list = []\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)\n",
    "dt_string = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "        epoch,train_loss=train(model, device, train_2_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n",
    "        test_loss,avg_cer,avg_wer=test(model, device, test_2_loader, criterion, epoch, iter_meter)\n",
    "        print('Train Epoch: {}: Average loss: {:.4f} Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}'.format(epoch,train_loss,test_loss, avg_cer, avg_wer))\n",
    "        \n",
    "        \n",
    "        \n",
    "        epoch_list.append(epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "        avg_cer_list.append(avg_cer)\n",
    "        avg_wer_list.append(avg_wer)\n",
    "        output = np.vstack((np.array(epoch_list),np.array(train_loss_list),np.array(test_loss_list),np.array(avg_cer_list),np.array(avg_wer_list))).T\n",
    "        \n",
    "        \n",
    "\n",
    "        with open(\"/media/kremon-storage-space-2/results/\"+dt_string+'.pickle', 'wb') as handle:\n",
    "             pickle.dump(output, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "        if epoch % 10 == 0:\n",
    "            print(\"-----------------------------------------------------\")\n",
    "            print(\"Saving Neural Network State\"+\"model_\"+dt_string+\"_\"+str(epoch)+\".pt\")\n",
    "            print(\"-----------------------------------------------------\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss\n",
    "                }, \"/media/kremon-storage-space-2/models/\"+dt_string+\"_\"+str(epoch)+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../models/\"+\"model_\"+dt_string+\"_\"+str(10)+\".pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepocessed_data=b[60:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = data.DataLoader(dataset=prepocessed_data,\n",
    "                                batch_size=1,\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, _data in enumerate(valid_loader):\n",
    "    print(i)\n",
    "    spectrograms, labels, input_lengths, label_lengths = _data\n",
    "    spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "    output = model(spectrograms)  # (batch, time, n_class)\n",
    "    output = F.log_softmax(output, dim=2)\n",
    "    output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "    decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "    print(decoded_preds)\n",
    "    print(decoded_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list=[1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_transform.int_to_text(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "            for i, _data in enumerate(test_loader):\n",
    "                spectrograms, labels, input_lengths, label_lengths = _data \n",
    "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "                output = model(spectrograms)  # (batch, time, n_class)\n",
    "                output = F.log_softmax(output, dim=2)\n",
    "                output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "                loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "                test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "                decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "                for j in range(len(decoded_preds)):\n",
    "                    test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                    test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "\n",
    "    avg_cer = sum(test_cer)/len(test_cer)\n",
    "    avg_wer = sum(test_wer)/len(test_wer)\n",
    "\n",
    "\n",
    "    return test_loss,avg_cer,avg_wer"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
